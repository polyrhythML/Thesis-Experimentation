{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73982a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "sys.path.append(\"./torchMoji\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from torch import nn\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from experiments.TorchHelper import TorchHelper\n",
    "from experiments import utils as U\n",
    "from models.model import *\n",
    "import experiments.config as C\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import copy\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random\n",
    "from torchMoji.torchmoji.word_generator import WordGenerator\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eebb43",
   "metadata": {},
   "source": [
    "### Try Torchmoji Vocab builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchMoji.torchmoji.word_generator import WordGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gen = WordGenerator(stream=df_train, ignore_emojis=False, allow_unicode_text=True)\n",
    "\n",
    "list_words = []\n",
    "maxm = None\n",
    "maxm_text = \"\"\n",
    "for df in [df_train, df_dev]:\n",
    "    for text in df[\"Text\"]:\n",
    "        if maxm is None:\n",
    "            maxm = len(word_gen.get_words(text))\n",
    "            maxm_text = text\n",
    "        else:\n",
    "            if len(word_gen.get_words(text)) > maxm:\n",
    "                maxm = len(word_gen.get_words(text))\n",
    "                maxm_text = text\n",
    "        \n",
    "        list_words += word_gen.get_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4af763",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the set of words from the corpus and build a vocab with key as the token and value as the index.\n",
    "vocab_dict = {}\n",
    "for idx, word in enumerate(set(list_words)):\n",
    "    vocab_dict[word] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc64850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_object = json.dumps(vocab_dict, indent = 4)\n",
    "with open(\"./preprocessing/vocab.json\", \"w\", encoding =\"utf-8\") as file:\n",
    "    file.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac03c1",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6b4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "train_path = \"./eng/trac2_eng_train.csv\"\n",
    "dev_path = \"./eng/trac2_eng_dev.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2dda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_dev = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efa1231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C45.451</td>\n",
       "      <td>Next part</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C47.11</td>\n",
       "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C33.79</td>\n",
       "      <td>🤣🤣😂😂🤣🤣🤣😂osm vedio ....keep it up...make more v...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4.1961</td>\n",
       "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C10.153</td>\n",
       "      <td>Concerned authorities should bring arundathi R...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               Text Sub-task A  \\\n",
       "0  C45.451                                          Next part        NAG   \n",
       "1   C47.11                 Iii8mllllllm\\nMdxfvb8o90lplppi0005        NAG   \n",
       "2   C33.79  🤣🤣😂😂🤣🤣🤣😂osm vedio ....keep it up...make more v...        NAG   \n",
       "3  C4.1961  What the fuck was this? I respect shwetabh and...        NAG   \n",
       "4  C10.153  Concerned authorities should bring arundathi R...        NAG   \n",
       "\n",
       "  Sub-task B  \n",
       "0       NGEN  \n",
       "1       NGEN  \n",
       "2       NGEN  \n",
       "3       NGEN  \n",
       "4       NGEN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4263 entries, 0 to 4262\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          4263 non-null   object\n",
      " 1   Text        4263 non-null   object\n",
      " 2   Sub-task A  4263 non-null   object\n",
      " 3   Sub-task B  4263 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 133.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_label_dist(df):\n",
    "    label_vals = []\n",
    "    labels = []\n",
    "    for taskA in df[\"Sub-task A\"].unique():\n",
    "        for taskB in df[\"Sub-task B\"].unique():\n",
    "            labels.append(f\"{taskA}-{taskB}\")\n",
    "            label_vals.append(len(df[(df[\"Sub-task A\"] == taskA) & (df[\"Sub-task B\"] == taskB)]))\n",
    "    \n",
    "    dist_dict = dict(zip(labels, label_vals))\n",
    "    print(dist_dict)\n",
    "    \n",
    "    df_labels_dist = pd.DataFrame({\"Label\": labels, \"Frequency\": label_vals})\n",
    "    \n",
    "    return df_labels_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1615374",
   "metadata": {},
   "source": [
    "### Training Data Label wise Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca620561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dist = gen_label_dist(df_train)\n",
    "df_train_dist.plot.bar(x='Label', y='Frequency', rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab3deb",
   "metadata": {},
   "source": [
    "### Dev Data Label wise Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_dist = gen_label_dist(df_dev)\n",
    "df_dev_dist.plot.bar(x='Label', y='Frequency', rot=0, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd60bf2",
   "metadata": {},
   "source": [
    "Clearly as we can see, there is data skewness for the datapoints of NAG-NGEN. The case of interest for us are CAG-GEN, OAG-GEN and NAG-GEN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b5ed3",
   "metadata": {},
   "source": [
    "## Manually Inspect the Gender Aggression Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c23df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAG - GEN\n",
    "# TRAIN\n",
    "train_oag_gen = df_train[(df_train[\"Sub-task A\"] == \"OAG\") & (df_train[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "train_oag_gen.to_csv(\"train_oag_gen.csv\")\n",
    "print(len(train_oag_gen))\n",
    "\n",
    "# DEV\n",
    "dev_oag_gen = df_dev[(df_dev[\"Sub-task A\"] == \"OAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "dev_oag_gen.to_csv(\"dev_oag_gen.csv\")\n",
    "print(len(dev_oag_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAG - GEN\n",
    "# TRAIN\n",
    "train_cag_gen = df_train[(df_train[\"Sub-task A\"] == \"CAG\") & (df_train[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "train_cag_gen.to_csv(\"train_cag_gen.csv\")\n",
    "print(len(train_cag_gen))\n",
    "\n",
    "# DEV\n",
    "dev_cag_gen = df_dev[(df_dev[\"Sub-task A\"] == \"CAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "dev_cag_gen.to_csv(\"dev_cag_gen.csv\")\n",
    "print(len(dev_cag_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25baf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAG - GEN\n",
    "# TRAIN\n",
    "train_nag_gen = df_train[(df_train[\"Sub-task A\"] == \"NAG\") & (df_train[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "train_nag_gen.to_csv(\"train_nag_gen.csv\")\n",
    "print(len(train_nag_gen))\n",
    "\n",
    "# DEV\n",
    "dev_nag_gen = df_dev[(df_dev[\"Sub-task A\"] == \"NAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")][\"Text\"]\n",
    "dev_nag_gen.to_csv(\"dev_nag_gen.csv\")\n",
    "print(len(dev_nag_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c876da",
   "metadata": {},
   "source": [
    "### Review and label the train and dev {OAG, CAG, NAG} - GEN texts \n",
    "\n",
    "* 1 - Misogyny \n",
    "\n",
    "* 0 - Neutral stance\n",
    "\n",
    "* Code switching \n",
    "\n",
    "* Code mixing\n",
    "\n",
    "There are typos also to be observed in some of the texts, as can be seen below : \n",
    "\n",
    "Train-Set\n",
    "\n",
    "\n",
    "* `All these urban naxaulites like Arundhati Roy have citizenship in other countries, My hindu brothers, where will u go? Fir shake of money they will sell us! What this Prustitute Arundhati Roy did for the nation's economy, science, technology, envirniment, poor...nothing!`\n",
    "\n",
    "\n",
    "There are cases where the person is making derogatory remark over wrong behaviour of women using strong words, should that be a genuine hate speech text or not is tough call to make. For example\n",
    "\n",
    "\n",
    "* `What's even more disgusting is that not 1 INDIAN WOMAN has the courage to post\n",
    "a comment against this....probably they fear the backlash....WHAT A SHAME ON\n",
    "SOCIETY!!!!! Fuck off you BITCHES!!!!!!!!!!!!!!!!!!!!!!!`\n",
    "\n",
    "\n",
    "* We will get the output for the code mixed and code switched gendered aggression cases too and see, how the model performs.\n",
    "\n",
    "\n",
    "* The Text which truly represents Female Gendered Aggression filtered out after manual review. Most of the textual artefacts are code switched and mixed. Would be interesting to see how the misogyny detection model performs over the same, but the issue would come in finding the sentiment values of those code switched words in the MESS computation. We will see, how to figure this out.\n",
    "\n",
    "\n",
    "* Clear Definition of the Misogyny is required as it can be seen in the dataset that Gendered aggressive comments targets male population too. Another point to be observed in the dataset, it has a very localised data collection which involves people from India commenting on the bollywood's feminism problem, or targetting politician for their double standard stances. It doesnot represent a global view of the misogynistic behavior.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reviwed Gen Aggression Files.\n",
    "df_rev_oag_gen_tr = pd.read_csv(\"./reviewed/train_oag_gen.csv\")\n",
    "df_rev_cag_gen_tr = pd.read_csv(\"./reviewed/train_cag_gen.csv\")\n",
    "df_rev_oag_gen_dev = pd.read_csv(\"./reviewed/dev_oag_gen.csv\")\n",
    "df_rev_cag_gen_dev = pd.read_csv(\"./reviewed/dev_cag_gen.csv\")\n",
    "\n",
    "list_rev_df = [\n",
    "    df_rev_oag_gen_tr, \n",
    "    df_rev_cag_gen_tr, \n",
    "    df_rev_oag_gen_dev, \n",
    "    df_rev_cag_gen_dev\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69769999",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cnt = []\n",
    "list_col_name = [\"rev_oag_gen_tr\", \"rev_cag_gen_tr\", \"rev_oag_gen_dev\", \"rev_cag_gen_dev\"]\n",
    "\n",
    "for df_rev in list_rev_df:\n",
    "    if \"Comment\" in df_rev.columns:\n",
    "        list_cnt.append(len(df_rev[df_rev[\"Comment\"] == '1']))\n",
    "    if \"Comments\" in df_rev.columns:\n",
    "        list_cnt.append(len(df_rev[df_rev[\"Comments\"] == 1]))\n",
    "\n",
    "df_rev_plot = pd.DataFrame({\"Rev-Data\": list_col_name, \"Frequency\": list_cnt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_plot.plot.bar(x='Rev-Data', y='Frequency', rot=0, color='red')\n",
    "plt.show()\n",
    "df_rev_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652d649",
   "metadata": {},
   "source": [
    "` SOME OF THE STATS AFTER THE REVIEW `\n",
    "\n",
    "\n",
    "Before Review :\n",
    "\n",
    "OAG-GEN(Train) = 140\n",
    "\n",
    "OAG-GEN(Dev) = 26\n",
    "\n",
    "\n",
    "After Review:\n",
    "\n",
    "OAG-GEN(Train) = 44\n",
    "\n",
    "OAG-GEN(Dev) = 7\n",
    "\n",
    "\n",
    "Before Review :\n",
    "\n",
    "CAG-GEN(Train) = 35\n",
    "\n",
    "CAG-GEN(Dev) = 9\n",
    "\n",
    "\n",
    "Before Review :\n",
    "\n",
    "CAG-GEN(Train) = 4\n",
    "\n",
    "CAG-GEN(Dev) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3dd83",
   "metadata": {},
   "source": [
    "###  Get the Model for the Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e195cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Creates and returns the model.\n",
    "    Moves to GPU if found any.\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = BERTRA()\n",
    "\n",
    "    torch_helper = TorchHelper() \n",
    "    \n",
    "    # /Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/weights/pretrained_models/eng_model_best.pth\n",
    "    dir_model = \"../weights/pretrained_models/eng_model_best.pth\"\n",
    "    \n",
    "    torch_helper.load_saved_model(model, dir_model)\n",
    "    print('model loaded')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014be35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loaded\n"
     ]
    }
   ],
   "source": [
    "def pad_features(docs_ints, seq_length=200):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "def masking(docs_ints, seq_length=200):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    masks = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        #mask[i, :len(row)] = 1\n",
    "        masks[i, -len(row):] = 1\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "# Load the preprocessed JSON.\n",
    "features_dev = json.load(open('./preprocessed_data/eng_dev.json'))\n",
    "\n",
    "dev_set = [val for key,val in features_dev.items()]\n",
    "print('Validation Loaded')\n",
    "\n",
    "test_set = dev_set[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d8b60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label-1</th>\n",
       "      <th>Label-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U deserve more subscribers. U really great.</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice video....</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorry if i bother somebody.. iam a defence asp...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>Gen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joker was amazing....it was not glamorised !.....</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nice baro</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>she telling to give the fake information it sh...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@username c whateva u said,is true........thes...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Love your review bhai , and movie awesome hai</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Randi Naxalist Roy</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I hate ranu mondal💀☠️👿👿👿</td>\n",
       "      <td>OAG</td>\n",
       "      <td>NGen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label-1 Label-2\n",
       "0        U deserve more subscribers. U really great.     NAG    NGen\n",
       "1                                     Nice video....     NAG    NGen\n",
       "2  sorry if i bother somebody.. iam a defence asp...     NAG     Gen\n",
       "3  Joker was amazing....it was not glamorised !.....     NAG    NGen\n",
       "4                                          Nice baro     NAG    NGen\n",
       "5  she telling to give the fake information it sh...     NAG    NGen\n",
       "6  @username c whateva u said,is true........thes...     CAG    NGen\n",
       "7      Love your review bhai , and movie awesome hai     NAG    NGen\n",
       "8                                 Randi Naxalist Roy     NAG    NGen\n",
       "9                           I hate ranu mondal💀☠️👿👿👿     OAG    NGen"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agg_labels = {0 : \"NAG\", 1: \"OAG\", 2: \"CAG\"}\n",
    "gen_labels ={0: \"NGen\", 1:\"Gen\"}\n",
    "\n",
    "\n",
    "def pred(model, test_df, agg_dict, gen_dict):\n",
    "    \n",
    "    batch_x, batch_t, batch_y1, batch_y2 = [], [], [], []\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        batch_x.append(test_df[i]['tokenized'])\n",
    "        batch_t.append(test_df[i]['DM'])\n",
    "\n",
    "        batch_y1.append(test_df[i]['y1'])\n",
    "        batch_y2.append(test_df[i]['y2'])\n",
    "\n",
    "    mask = masking(batch_x)\n",
    "    padded = pad_features(batch_x)\n",
    "\n",
    "    out = model.forward(torch.tensor(padded), torch.tensor(mask))\n",
    "\n",
    "    y_pred1 = out['y_pred1'].cpu()\n",
    "    y_pred2 = out['y_pred2'].cpu()\n",
    "    \n",
    "    pred_agg_labels = [agg_dict[val] for val in np.argmax(y_pred1.detach().numpy(), axis=1)]\n",
    "    pred_gen_labels = [gen_dict[val] for val in np.argmax(y_pred2.detach().numpy(), axis=1)]\n",
    "    \n",
    "    # Return DataFrame with \n",
    "    # Columns 1: Text, Column 2: Label-1, Column 3: Label-2\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Text\": [test_df[i]['post'] for i in range(len(test_df))],\n",
    "        \"Label-1\": pred_agg_labels,\n",
    "        \"Label-2\": pred_gen_labels\n",
    "                           })\n",
    "    \n",
    "    return df_pred\n",
    "\n",
    "\n",
    "pred(model, test_set, agg_labels, gen_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6274d",
   "metadata": {},
   "source": [
    "### Pointers to Observe for the model training:\n",
    "\n",
    "* The tokenized key -> comes from the BERT tokenizer\n",
    "* DM -> The token ids from our vocab generated using custom built vocab.\n",
    "* DM is not used for the inference neither, it is used in the training.\n",
    "* The author never used a custom built vocab for the purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e877c33",
   "metadata": {},
   "source": [
    "## Sub Filter the Train and Dev Test based on the Label Space and Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08a2b0",
   "metadata": {},
   "source": [
    "### OAG-GEN-DEV SET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc31d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_rev_oag_gen_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/94/h8fqj0tj6q195tq2yh03z4ts4k11ty/T/ipykernel_3019/3943098211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# filter the datapoint which were actually aggressive and genered.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf_oag_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_rev_oag_gen_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_rev_oag_gen_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Comments\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Convert the above filtered set into model features ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_rev_oag_gen_dev' is not defined"
     ]
    }
   ],
   "source": [
    "# Output Prediction for the Datapoint where the manual review declared it as Actually aggressive and Gendered\n",
    "\n",
    "dev_set_feat = [val for key,val in features_dev.items()]\n",
    "\n",
    "\n",
    "def filter_feat(ft_list, df):\n",
    "    list_ft = []\n",
    "    for text in df[\"Text\"]:\n",
    "        for ft in ft_list:\n",
    "            if ft[\"post\"] == text:\n",
    "                list_ft.append(ft)\n",
    "                break\n",
    "    \n",
    "    return list_ft\n",
    "\n",
    "# filter the datapoint which were actually aggressive and genered.\n",
    "df_oag_filter = df_rev_oag_gen_dev[df_rev_oag_gen_dev[\"Comments\"] == 1]\n",
    "\n",
    "# Convert the above filtered set into model features ...\n",
    "test_set = filter_feat(dev_set_feat, df_oag_filter)\n",
    "\n",
    "# Perform the prediction on top of the model ... \n",
    "pred(model, test_set, agg_labels, gen_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b336f",
   "metadata": {},
   "source": [
    "### CAG-GEN-DEV SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb234e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_cag_gen_dev\n",
    "dev_set = [val for key,val in features_dev.items()]\n",
    "\n",
    "\n",
    "def filter_feat(ft_list, df):\n",
    "    list_ft = []\n",
    "    for text in df[\"Text\"]:\n",
    "        for ft in ft_list:\n",
    "            if ft[\"post\"] == text:\n",
    "                list_ft.append(ft)\n",
    "                break\n",
    "    \n",
    "    return list_ft\n",
    "\n",
    "df_rev_dev = df_rev_cag_gen_dev[df_rev_cag_gen_dev[\"Comments\"] == 1]\n",
    "\n",
    "test_set = filter_feat(dev_set, df_rev_cag_gen_dev)\n",
    "pred(model, test_set, agg_labels, gen_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3516f",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839f75f",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c4d45",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "\n",
    "1. Take the dev set. We will move to the train later, first check on the datapoints where the distribution is unseen. \n",
    "2. Filter on the NAG, OAG, CAG and Gen-NonGen.\n",
    "3. First Preference - Gendered + {NAG, OAG, CAG}.\n",
    "4. Perform the Inference on top of the set and filter the exact match and Non-Match.\n",
    "5. Exact Match are the cases where the model is doing exactly as per expected. -> Here we can also check the probability value for that particular label. How confident the model seems.\n",
    "6. Take the above set and generate the Explanation using HEDGE.\n",
    "7. Take the output of the explanation weights and the sentiment values from the sentiment vocab -> generate MESS.\n",
    "8. Now go back and take up the mismatch set, where the label gave wrong prediction and then perform the steps 5 -7.\n",
    "9. If the explanation model generate high weight emphasis for the correct set of words but gives a wrong label, this means model is performing some shortcut learning cues inside to be able to come-up with actual prediction. - We will have to see.. But this case is interesting from the perspective... did model give wrong label for the right reasons for wrong label bcos of the wrong reasons.. again subject to the explanation algorithm that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaeb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Dev Set\n",
    "\n",
    "dev_path = \"eng/trac2_eng_dev.csv\"\n",
    "df_dev = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30bb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate OAG-GEN, CAG-GEN and NAG-GEN\n",
    "dev_OG = df_dev[(df_dev[\"Sub-task A\"] == \"OAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")]\n",
    "dev_CG = df_dev[(df_dev[\"Sub-task A\"] == \"CAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")]\n",
    "dev_NG = df_dev[(df_dev[\"Sub-task A\"] == \"NAG\") & (df_dev[\"Sub-task B\"] == \"GEN\")]\n",
    "\n",
    "\n",
    "# Segregate OAG-NGEN, CAG-NGEN, NAG-NGEN\n",
    "dev_ONG = df_dev[(df_dev[\"Sub-task A\"] == \"OAG\") & (df_dev[\"Sub-task B\"] == \"NGEN\")]\n",
    "dev_CNG = df_dev[(df_dev[\"Sub-task A\"] == \"CAG\") & (df_dev[\"Sub-task B\"] == \"NGEN\")]\n",
    "dev_NNG = df_dev[(df_dev[\"Sub-task A\"] == \"NAG\") & (df_dev[\"Sub-task B\"] == \"NGEN\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_labels = {0 : \"NAG\", 1: \"OAG\", 2: \"CAG\"}\n",
    "gen_labels = {0: \"NGEN\", 1:\"GEN\"}\n",
    "\n",
    "\n",
    "def pred(model, test_df, agg_dict, gen_dict):\n",
    "    \n",
    "    batch_x, batch_t, batch_y1, batch_y2 = [], [], [], []\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        batch_x.append(test_df[i]['tokenized'])\n",
    "        batch_t.append(test_df[i]['DM'])\n",
    "\n",
    "        batch_y1.append(test_df[i]['y1'])\n",
    "        batch_y2.append(test_df[i]['y2'])\n",
    "\n",
    "    mask = masking(batch_x)\n",
    "    padded = pad_features(batch_x)\n",
    "\n",
    "    out = model.forward(torch.tensor(padded), torch.tensor(mask))\n",
    "\n",
    "    y_pred1 = out['y_pred1'].cpu()\n",
    "    y_pred2 = out['y_pred2'].cpu()\n",
    "    \n",
    "    pred_agg_prob = [val for val in y_pred1.detach().numpy()]\n",
    "    pred_gen_prob = [val for val in y_pred2.detach().numpy()]\n",
    "    \n",
    "    pred_agg_labels = [agg_dict[val] for val in np.argmax(y_pred1.detach().numpy(), axis=1)]\n",
    "    pred_gen_labels = [gen_dict[val] for val in np.argmax(y_pred2.detach().numpy(), axis=1)]\n",
    "    \n",
    "    \n",
    "    # Return DataFrame with \n",
    "    # Columns 1: Text, Column 2: Label-1, Column 3: Label-2\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Text\": [test_df[i]['post'] for i in range(len(test_df))],\n",
    "        \"Label-1\": pred_agg_labels,\n",
    "        \"Label-2\": pred_gen_labels,\n",
    "        \"Agg-Prob\": pred_agg_prob,\n",
    "        \"Gen-Prob\": pred_gen_prob\n",
    "                           })\n",
    "    \n",
    "    return df_pred\n",
    "\n",
    "\n",
    "def filter_feat(ft_list, df):\n",
    "    list_ft = []\n",
    "    for text in df[\"Text\"]:\n",
    "        for ft in ft_list:\n",
    "            if ft[\"post\"] == text:\n",
    "                list_ft.append(ft)\n",
    "                break\n",
    "    \n",
    "    return list_ft\n",
    "\n",
    "\n",
    "def pad_features(docs_ints, seq_length=200):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "def masking(docs_ints, seq_length=200):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    masks = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        #mask[i, :len(row)] = 1\n",
    "        masks[i, -len(row):] = 1\n",
    "\n",
    "    return masks\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Creates and returns the model.\n",
    "    Moves to GPU if found any.\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = BERTRA()\n",
    "\n",
    "    torch_helper = TorchHelper()\n",
    "    \n",
    "    # /Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/weights/pretrained_models/eng_model_best.pth\n",
    "    dir_model = \"../weights/pretrained_models/eng_model_best.pth\"\n",
    "    \n",
    "    torch_helper.load_saved_model(model, dir_model)\n",
    "    print('Model loaded')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def filterMatchPred(df_gt, df_pred):\n",
    "    gt_list = []\n",
    "    for text in df_pred[\"Text\"]:\n",
    "        gt_label = df_gt[df_gt[\"Text\"] == text][\"GT-Label\"].values[0]\n",
    "        gt_list.append(gt_label)\n",
    "    \n",
    "    df_pred[\"GT-Label\"] = gt_list\n",
    "    \n",
    "    # Instances where, there is exact match of the pred and GT.\n",
    "    # np.where(df[\"col1\"] == df[\"col2\"], True, False)\n",
    "\n",
    "    idxs = df_pred[\"Pred-Label\"].eq(df_pred[\"GT-Label\"])\n",
    "    \n",
    "    df_filter = df_pred[idxs]\n",
    "\n",
    "    return df_pred, df_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d6560",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-OAG-GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42beef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dev_OG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/94/h8fqj0tj6q195tq2yh03z4ts4k11ty/T/ipykernel_3019/1961830711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_OG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpred_dev_OG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_OG' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate Prediction based on the\n",
    "\n",
    "# Load the Feature Set of the Dev Set \n",
    "# Load the preprocessed JSON.\n",
    "features_dev = json.load(open('./preprocessed_data/eng_dev.json'))\n",
    "dev_ft = [val for key,val in features_dev.items()]\n",
    "\n",
    "# Load the Misogyny Detection Model\n",
    "model = load_model()\n",
    "\n",
    "test_set = filter_feat(dev_ft, dev_OG)\n",
    "pred_dev_OG = pred(model, test_set, agg_labels, gen_labels)\n",
    "\n",
    "# Concatentate the Label-1 and Label-2 Together\n",
    "pred_dev_OG[\"Pred-Label\"] = pred_dev_OG[\"Label-1\"] + \"-\" + pred_dev_OG[\"Label-2\"]\n",
    "pred_dev_OG.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "pred_dev_OG = pred_dev_OG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n",
    "pred_dev_OG.info()\n",
    "\n",
    "pred_dev_OG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe2ca",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-CAG-GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filter_feat(dev_ft, dev_CG)\n",
    "pred_dev_CG = pred(model, test_set, agg_labels, gen_labels)\n",
    "# Concatentate the Label-1 and Label-2 Together\n",
    "pred_dev_CG[\"Pred-Label\"] = pred_dev_CG[\"Label-1\"] + \"-\" + pred_dev_CG[\"Label-2\"]\n",
    "pred_dev_CG.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "pred_dev_CG = pred_dev_CG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n",
    "pred_dev_CG.info()\n",
    "pred_dev_CG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcb190",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-NAG-GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a49d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filter_feat(dev_ft, dev_NG)\n",
    "pred_dev_NG = pred(model, test_set, agg_labels, gen_labels)\n",
    "\n",
    "# Concatentate the Label-1 and Label-2 Together\n",
    "pred_dev_NG[\"Pred-Label\"] = pred_dev_NG[\"Label-1\"] + \"-\" + pred_dev_NG[\"Label-2\"]\n",
    "pred_dev_NG.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "pred_dev_NG = pred_dev_NG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n",
    "pred_dev_NG.info()\n",
    "pred_dev_NG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeaca68",
   "metadata": {},
   "source": [
    "### Filter Prediction Matching with GT and Not Matching with GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev set -> GT Label made consistent with the Predicted Label\n",
    "if \"Sub-task A\" in df_dev:\n",
    "    df_dev[\"GT-Label\"] = df_dev[\"Sub-task A\"] + \"-\" + df_dev[\"Sub-task B\"]\n",
    "    df_dev.drop([\"Sub-task A\", \"Sub-task B\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9758c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeea876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter OG\n",
    "df_pred_OG, df_filter_OG = filterMatchPred(df_dev, pred_dev_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616540b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_pred_OG))\n",
    "print(len(df_filter_OG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52959c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter CG\n",
    "df_pred_CG, df_filter_CG = filterMatchPred(df_dev, pred_dev_CG)\n",
    "print(len(df_pred_CG))\n",
    "print(df_filter_CG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter NG\n",
    "df_pred_NG, df_filter_NG = filterMatchPred(df_dev, pred_dev_NG)\n",
    "print(len(df_pred_NG))\n",
    "print(len(df_filter_NG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05f27d",
   "metadata": {},
   "source": [
    "**Match Statistics for the Gendered Dev Set**\n",
    "\n",
    "Out of total 22 OG instances - 1 Matches - 4.5% Accuracy\n",
    "\n",
    "Out of total 9 CG instances - 0 Matches - 0 % Accuracy\n",
    "\n",
    "Out of total 34 NG instances - 5 Matches - 5 % Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f529d",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-OAG-NGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filter_feat(dev_ft, dev_ONG)\n",
    "pred_dev_ONG = pred(model, test_set, agg_labels, gen_labels)\n",
    "\n",
    "# Concatentate the Label-1 and Label-2 Together\n",
    "pred_dev_ONG[\"Pred-Label\"] = pred_dev_ONG[\"Label-1\"] + \"-\" + pred_dev_ONG[\"Label-2\"]\n",
    "pred_dev_ONG.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "pred_dev_ONG = pred_dev_ONG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n",
    "pred_dev_ONG.info()\n",
    "\n",
    "pred_dev_ONG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1fb72",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-CAG-NGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d23672",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filter_feat(dev_ft, dev_CNG)\n",
    "pred_dev_CNG = pred(model, test_set, agg_labels, gen_labels)\n",
    "\n",
    "# Concatentate the Label-1 and Label-2 Together\n",
    "pred_dev_CNG[\"Pred-Label\"] = pred_dev_CNG[\"Label-1\"] + \"-\" + pred_dev_CNG[\"Label-2\"]\n",
    "pred_dev_CNG.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "pred_dev_CNG = pred_dev_CNG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n",
    "pred_dev_CNG.info()\n",
    "pred_dev_CNG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7292e89",
   "metadata": {},
   "source": [
    "### Misogyny Detection Inference over DEV-NAG-NGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe48295",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filter_feat(dev_ft, dev_NNG)\n",
    "\n",
    "df_pred_list = []\n",
    "total_ele = 0 \n",
    "\n",
    "for idx in range(8):\n",
    "\n",
    "    if idx < 7:\n",
    "        test_sub = test_set[idx*100: (idx+1)*100]\n",
    "    else:\n",
    "        test_sub = test_set[idx*100: ]\n",
    "    \n",
    "    pred_dev_NNG_sub = pred(model, test_sub, agg_labels, gen_labels)\n",
    "\n",
    "    # Concatentate the Label-1 and Label-2 Together\n",
    "    pred_dev_NNG_sub[\"Pred-Label\"] = pred_dev_NNG_sub[\"Label-1\"] + \"-\" + pred_dev_NNG_sub[\"Label-2\"]\n",
    "    pred_dev_NNG_sub.drop([\"Label-1\", \"Label-2\"], axis=1, inplace=True)\n",
    "    \n",
    "    df_pred_list.append(pred_dev_NNG_sub)\n",
    "    total_ele += len(pred_dev_NNG_sub)\n",
    "\n",
    "pred_dev_NNG = pd.concat(df_pred_list)\n",
    "pred_dev_NNG = pred_dev_NNG[[\"Text\", \"Pred-Label\", \"Agg-Prob\", \"Gen-Prob\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dbb01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    pred_dev_ONG.to_csv(\"pred_dev_ONG.csv\")\n",
    "    pred_dev_CNG.to_csv(\"pred_dev_CNG.csv\")\n",
    "    pred_dev_NNG.to_csv(\"pred_dev_NNG.csv\")\n",
    "    pred_dev_OG.to_csv(\"pred_dev_OG.csv\")\n",
    "    pred_dev_CG.to_csv(\"pred_dev_CG.csv\")\n",
    "    pred_dev_NG.to_csv(\"pred_dev_NG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a769403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv(\"./model_outputs/dev-prediction/pred_dev_NG.csv\")\n",
    "print(len(x))\n",
    "len(x[x[\"Pred-Label\"] == x[\"GT-Label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c172a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_dev_NNG.info())\n",
    "pred_dev_NNG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev_NNG.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705568c0",
   "metadata": {},
   "source": [
    "### Filter Prediction Matching with GT and Not Matching with GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5180464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ONG\n",
    "df_pred_ONG, df_filter_ONG = filterMatchPred(df_dev, pred_dev_ONG)\n",
    "\n",
    "# Filter CNG\n",
    "df_pred_CNG, df_filter_CNG = filterMatchPred(df_dev, pred_dev_CNG)\n",
    "\n",
    "# Filter NNG\n",
    "df_pred_NNG, df_filter_NNG = filterMatchPred(df_dev, pred_dev_NNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Store the CSVs\n",
    "if 0:\n",
    "    pred_dev_ONG.to_csv(\"./model_outputs/dev-prediction/pred_dev_ONG.csv\")\n",
    "    pred_dev_CNG.to_csv(\"./model_outputs/dev-prediction/pred_dev_CNG.csv\")\n",
    "    pred_dev_NNG.to_csv(\"./model_outputs/dev-prediction/pred_dev_NNG.csv\")\n",
    "    pred_dev_OG.to_csv(\"./model_outputs/dev-prediction/pred_dev_OG.csv\")\n",
    "    pred_dev_CG.to_csv(\"./model_outputs/dev-prediction/pred_dev_CG.csv\")\n",
    "    pred_dev_NG.to_csv(\"./model_outputs/dev-prediction/pred_dev_NG.csv\")\n",
    "\n",
    "# Store filtered DataFrames\n",
    "if 0:\n",
    "    df_filter_CNG.to_csv(\"./model_outputs/filt-dev-pred/filter_CNG.csv\")\n",
    "    df_filter_ONG.to_csv(\"./model_outputs/filt-dev-pred/filter_ONG.csv\")\n",
    "    df_filter_NNG.to_csv(\"./model_outputs/filt-dev-pred/filter_NNG.csv\")\n",
    "    df_filter_CG.to_csv(\"./model_outputs/filt-dev-pred/filter_CG.csv\")\n",
    "    df_filter_OG.to_csv(\"./model_outputs/filt-dev-pred/filter_OG.csv\")\n",
    "    df_filter_NG.to_csv(\"./model_outputs/filt-dev-pred/filter_NG.csv\")\n",
    "\n",
    "\n",
    "if 0:    \n",
    "    # Formate the Filtered DataFrames to input to the Hedge Model\n",
    "    names = [\"filt_ONG\", \"filt_CNG\", \"filt_NNG\", \"filt_OG\", \"filt_CG\", \"filt_NG\"]\n",
    "\n",
    "    for i, test in enumerate([df_filter_ONG, df_filter_CNG, df_filter_NNG, df_filter_OG, df_filter_CG, df_filter_NG]):\n",
    "\n",
    "        test = test[[\"Text\", \"Pred-Label\"]]\n",
    "        test.rename(columns={\"Text\": \"sentence\", \"Pred-Label\":\"label\"}, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        for idx in range(len(test)):\n",
    "            test[\"sentence\"][idx] = test[\"sentence\"][idx].replace('\\n', '')\n",
    "            if \"NGEN\" in test[\"label\"][idx]:\n",
    "                test[\"label\"][idx] = 0\n",
    "\n",
    "            else:\n",
    "                test[\"label\"][idx] = 1\n",
    "\n",
    "        test.to_csv(os.path.join(\"./model_outputs/dev-format-pred\", names[i] +\".tsv\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d054a",
   "metadata": {},
   "source": [
    "**The Statistics for the Non Gendered Subsets over the Dev Set**\n",
    "\n",
    "ONG : 7 : 77 -> 9% Acc\n",
    "\n",
    "CNG : 4 : 97 -> 4% Acc\n",
    "\n",
    "NNG : 616 : 714 -> 86% Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e99edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"eng/trac2_eng_train.csv\"\n",
    "df_train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Sub-task A\" in df_train.columns:\n",
    "    df_train[\"GT-Label\"] = df_train[\"Sub-task A\"] + \"-\" + df_train[\"Sub-task B\"]\n",
    "    df_train.drop([\"Sub-task A\", \"Sub-task B\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax = df_dev[\"GT-Label\"].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.tick_params(axis='x', which='major', labelsize=7, rotation=0)\n",
    "fig.savefig(\"fgs.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46db77",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "\n",
    "1. Create a function which connect with the Hedge Algorithm and extracts the explanation scores.\n",
    "\n",
    "2. For the cases where we have the exact Match of GT  with Pred - Generate the Explanation - Gendered\n",
    "More Gendered Case, the important factor is to generate filter Female gender - Since that contributes to misogyny.\n",
    "\n",
    "3. Then Generate the above results for the NGEN category of Datapoints where the GT matches with Pred\n",
    "\n",
    "4. For the cases where the model output doesnot match with GT, check the probability values.. that how off the model is from the actual label in terms of the probability value.\n",
    "\n",
    "5. Pointer 3 needs to be done overall for all the datapoints. w.r.t to the GT class. Per class how confident the model is at an average. GEN, NGEN class -> how confident the model is distribution of the all three label scores per datapoint in the task-1 and task-2.\n",
    "\n",
    "6. Visualization - Correlation Sentiment with Explanation sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e0d42",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64258264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the Text into sub text and then feed it to the HEDGE system for explanation.\n",
    "path = \"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-format-pred/filt_NNG.tsv\"\n",
    "\n",
    "df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "df[\"sentence\"][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sent = [\n",
    "    \"U deserve more subscribers. U really great.\",\n",
    "    \"Joker was amazing.it was not glamorised but Kabir Singh was tatti 100X\",\n",
    "    \"Love your review bhai , and movie awesome hai\",\n",
    "    \"She must be sent to jail for anti national activities under NSA and PSA\",\n",
    "    \"Excellent job...if law can't give you Justice....take it by yourself\",\n",
    "    \"I'm also a lesbian girl and I'm proud of myself\",\n",
    "    \"Arundhati's hairstyle is inspired by Dr. Mashahoor Gulati.\",\n",
    "    \"Arrested for what? Know a little about our Constitution. Let's see who will arrest her.\",\n",
    "    \"Please do a review on joker\",\n",
    "    \"That's her pick up line that nobody listens\",\n",
    "    \"Most honest and elaborate review thank u sir from GUJARAT. JAI SHRI RAM\",\n",
    "    \"very well done salute to this guy\",\n",
    "    \"Sir please upload the names of books which you read to develop mindset\",\n",
    "    \"thankyou for the quote that u put back there it helped me.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"sentence\"] = list_sent\n",
    "df[\"label\"] = [0 for _ in range(14)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1740b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df07d6",
   "metadata": {},
   "source": [
    "**Pointer for the document - 11/05/2022**\n",
    "\n",
    "1. Dev Set Statistics and train set Statistics\n",
    "2. Show that Dev set prediction has higher number of data points being predicted of NG class. Reason being - Data imbalance in the train data.\n",
    "3. Generate a plot of the prediction probability values for the class OG, CG, NG and ONG, CNG, and NNG. and check how much the model was confident for the GT class compared to the other classe. Indicates the confusion or clarity of the decision making of the model.\n",
    "4. Take the Filtered out cases from the (OG, CG, NG) and (ONG, CNG, NNG). - Take about the format transition to run the HEDGE model.\n",
    "5. Break down these sentence into more atomic sentences - reason being the computational intractability of the HEDGE run.\n",
    "6. Generate the HEDGE explanation score for each of the sentence in the subsets. \n",
    "7. Manually review all the datapoints - Plot 2 examples from each of the subsets consistent with the explanation and anomalous in the explanation.\n",
    "\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab13ab4",
   "metadata": {},
   "source": [
    "# Misogyny and Aggression Model Inference Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139ebed",
   "metadata": {},
   "source": [
    "Here we will check how much clarity the model has in predicting the right class, more the difference between prediction probabilities and towards the right class, better clarity the model has in terms of the \n",
    "prediction made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3a9c2",
   "metadata": {},
   "source": [
    "## Aggression Inference Analysis - Matching Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef22df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inference files\n",
    "df_OG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_OG.csv\")\n",
    "df_CG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_CG.csv\")\n",
    "df_NG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_NG.csv\")\n",
    "df_ONG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_ONG.csv\")\n",
    "df_CNG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_CNG.csv\")\n",
    "df_NNG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_NNG.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3841989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseOvertly(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[(df[\"Pred-Label\"].apply(lambda x : \"OAG\" in x)) & (df[\"Pred-Label\"] == df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[1] - probs[2])\n",
    "        list_diffs.append(probs[1] - probs[0])\n",
    "        \n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y3 = [val[2] for val in list_scores]\n",
    "    \n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2.5, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + width+0.1, y3, width=width, label='Covertly')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Overtly-Match.png\")\n",
    "        \n",
    "    return list_scores, list_diffs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ec51a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03328269, 0.78107005, 0.1856473]]\n",
      "[0.59542275, 0.74778736]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.671605055"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseOvertly(df_OG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed007385",
   "metadata": {},
   "source": [
    "**As clearly can be seen, the mean difference between the Probabilities for the Overtly class, other Covertly class and Non Aggression class is 0.67. It is quite a high number to claim that the decision making for the model among the other classes is way less ambiguous.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06390ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseCovertly(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"CAG\" in x) & (df[\"Pred-Label\"] == df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[2] - probs[1])\n",
    "        list_diffs.append(probs[2] - probs[0])\n",
    "    \n",
    "    \n",
    "    # Plot CO\n",
    "    x = np.arange(len(list_scores))\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y3 = [val[2] for val in list_scores]\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + width/2, y2, width=width, label='Covertly')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Covertly-Match.png\")\n",
    "    \n",
    "    \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1c789dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseCovertly(df_CG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5517998",
   "metadata": {},
   "source": [
    "**There were no data points for which the output of the misogyny aggression matches with the GT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d5a944a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseNoagg(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NAG\" in x)& (df[\"Pred-Label\"] == df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        list_diffs.append(probs[0] - probs[2])\n",
    "        \n",
    "        \n",
    "    # Plot NO\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y3 = [val[2] for val in list_scores]\n",
    "    \n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + width+0.1, y3, width=width, label='Covertly')\n",
    "    \n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/NonAgg-Match.png\")    \n",
    "    \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fb0213e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81337935, 0.10682596, 0.07979469], [0.7449015, 0.08207034, 0.17302823], [0.55568665, 0.2614945, 0.18281882], [0.83159167, 0.08601627, 0.08239194], [0.7066269, 0.11711952, 0.1762536]]\n",
      "[0.70655339, 0.73358466, 0.66283116, 0.57187327, 0.29419214999999993, 0.37286782999999996, 0.7455754, 0.74919973, 0.5895073799999999, 0.5303732999999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5956558269999999"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseNoagg(df_NG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004b5c5",
   "metadata": {},
   "source": [
    "## Aggression Inference Analysis - Non Matching Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "684110f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseOvertly(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"OAG\" not in x) & (df[\"Pred-Label\"] != df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[1] - probs[2])\n",
    "        list_diffs.append(probs[1] - probs[0])\n",
    "\n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y3 = [val[2] for val in list_scores]\n",
    "    \n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + width+0.1, y3, width=width, label='Covertly')\n",
    "    \n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Overtly-NonMatch.png\")\n",
    "\n",
    "    \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d0be774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8843626, 0.02343824, 0.09219919], [0.4688446, 0.11857922, 0.4125761], [0.9919593, 0.00197632, 0.00606437], [0.69011015, 0.18602732, 0.12386259], [0.98343223, 0.00516575, 0.01140205], [0.9189583, 0.02567236, 0.05536939], [0.5589883, 0.15123467, 0.28977704], [0.92511237, 0.03537672, 0.0395109], [0.6387676, 0.29541448, 0.06581795], [0.71394944, 0.10214195, 0.18390869], [0.09192161, 0.44985652, 0.45822185], [0.98096746, 0.00976037, 0.00927225], [0.46854386, 0.35010877, 0.18134733], [0.9915154, 0.00405018, 0.0044344], [0.90520763, 0.06229217, 0.03250017], [0.98266923, 0.00351752, 0.01381334], [0.8894454, 0.04938887, 0.06116574], [0.6378985, 0.16123861, 0.20086282], [0.75712156, 0.12016895, 0.12270953]]\n",
      "[-0.06876095, -0.86092436, -0.29399688, -0.35026538, -0.00408805, -0.98998298, 0.06216473, -0.50408283, -0.0062363, -0.97826648, -0.029697029999999996, -0.8932859400000001, -0.13854237, -0.40775363, -0.004134180000000001, -0.88973565, 0.22959652999999997, -0.34335312, -0.08176674000000002, -0.61180749, -0.008365330000000004, 0.35793491, 0.0004881200000000016, -0.97120709, 0.16876144, -0.11843508999999997, -0.0003842200000000002, -0.98746522, 0.029792, -0.8429154600000001, -0.01029582, -0.9791517099999999, -0.011776870000000002, -0.84005653, -0.03962420999999999, -0.47665989000000003, -0.0025405800000000006, -0.63695261]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.32983613842105264"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseOvertly(df_OG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa1933f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseCovertly(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"CAG\" not in x) & (df[\"Pred-Label\"] != df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[2] - probs[1])\n",
    "        list_diffs.append(probs[2] - probs[0])\n",
    "    \n",
    "    # Plot NO\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y2 = [val[2] for val in list_scores]\n",
    "    \n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + width+0.1, y2, width=width, label='Covertly')\n",
    "    \n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Covertly-NonMatch.png\")\n",
    "    \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff5fc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29887846, 0.3957506, 0.30537093], [0.4079646, 0.36466235, 0.22737305], [0.89669734, 0.03689234, 0.06641035], [0.7931357, 0.16030382, 0.04656041], [0.94362795, 0.02596619, 0.03040579], [0.59996796, 0.1951777, 0.20485431], [0.49739638, 0.16845286, 0.3341508], [0.878064, 0.06312154, 0.05881455], [0.9016908, 0.05072821, 0.04758093]]\n",
      "[-0.09037967000000002, 0.006492469999999972, -0.13728930000000003, -0.18059155000000002, 0.029518010000000004, -0.8302869900000001, -0.11374341000000002, -0.74657529, 0.004439599999999998, -0.91322216, 0.009676610000000002, -0.39511365, 0.16569794000000002, -0.16324558, -0.004306990000000004, -0.8192494499999999, -0.0031472800000000023, -0.85410987]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2797464755555556"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseCovertly(df_CG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5b97871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseNoagg(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    #corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NAG\" not in x) & (df[\"Pred-Label\"] != df[\"GT-Label\"])]\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NAG\" not in x)]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Agg-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1], probs[2]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        list_diffs.append(probs[0] - probs[2])\n",
    "\n",
    "    # Plot NO\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    y3 = [val[2] for val in list_scores]\n",
    "    \n",
    "    width = 0.25\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2.2, y1, width=width, label='Non-Agg')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Overtly')\n",
    "    rect3 = ax.bar(x + (width+0.12), y3, width=width, label='Covertly')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Nonagg-NonMatch.png\")    \n",
    "    \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05efd2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pred-Label</th>\n",
       "      <th>Agg-Prob</th>\n",
       "      <th>Gen-Prob</th>\n",
       "      <th>GT-Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry if i bother somebody.. iam a defence asp...</td>\n",
       "      <td>NAG-GEN</td>\n",
       "      <td>[0.81337935 0.10682596 0.07979469]</td>\n",
       "      <td>[0.20062688 0.79937315]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Randi Naxalist Roy</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.96015495 0.02288608 0.01695899]</td>\n",
       "      <td>[0.9624128 0.0375873]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wow.... india is progressing day by day.... No...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.9327383  0.02143447 0.04582729]</td>\n",
       "      <td>[0.99649435 0.00350569]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ARUNDHATI ROY.....HA HA HA........HER PARENTS ...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.84490246 0.03739336 0.11770418]</td>\n",
       "      <td>[0.9980465  0.00195348]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Allah and Muhammad can now have sex in India</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.90352356 0.02240491 0.07407154]</td>\n",
       "      <td>[0.9886021  0.01139788]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>World Knows The Prime Roots of Urban NAXAL tha...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.9047116  0.03067101 0.06461739]</td>\n",
       "      <td>[0.99661404 0.00338593]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Arundhati real name is Suzan and she is charis...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.9034141  0.04429049 0.05229545]</td>\n",
       "      <td>[0.9945122  0.00548781]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>We are here because our ancestors were against...</td>\n",
       "      <td>NAG-GEN</td>\n",
       "      <td>[0.7449015  0.08207034 0.17302823]</td>\n",
       "      <td>[0.09623028 0.90376973]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Homosexual hona is not a crime but it shouldn'...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.95881957 0.01758637 0.02359411]</td>\n",
       "      <td>[0.9765638  0.02343618]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>+onthebookshelf  \\nSexual relationships betwee...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.8022871  0.05470058 0.14301236]</td>\n",
       "      <td>[0.8259745  0.17402545]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>That's true brother. Seeing feminists butthurt...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.4651227  0.25265834 0.28221893]</td>\n",
       "      <td>[0.9833097  0.01669028]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>kapil why are u listening to these chutiaassss...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.57544947 0.19784397 0.22670656]</td>\n",
       "      <td>[0.98515826 0.01484168]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>This is the procedure every where , be it supe...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.8943346  0.03550828 0.07015709]</td>\n",
       "      <td>[0.5750906  0.42490944]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Hot</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.991954   0.00293526 0.00511077]</td>\n",
       "      <td>[9.9975806e-01 2.4197616e-04]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>There are only 2 genders</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.4688868  0.25112578 0.27998734]</td>\n",
       "      <td>[0.60404044 0.39595956]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>She is absolutely xxx movie partner even this ...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.9315344  0.03082788 0.03763773]</td>\n",
       "      <td>[0.98365027 0.01634971]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Homosexuality is disgusting. It should be cond...</td>\n",
       "      <td>NAG-GEN</td>\n",
       "      <td>[0.55568665 0.2614945  0.18281882]</td>\n",
       "      <td>[0.11387955 0.88612044]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>**We can 't allow to pollute our bravery envir...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.7579064  0.0876889  0.15440473]</td>\n",
       "      <td>[0.9662474  0.03375266]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Freedom of expression. Kasthuri it’s bullshit....</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.5485263  0.04553279 0.40594092]</td>\n",
       "      <td>[0.99061877 0.00938131]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Homosexuality should be allowed in the army......</td>\n",
       "      <td>NAG-GEN</td>\n",
       "      <td>[0.83159167 0.08601627 0.08239194]</td>\n",
       "      <td>[0.08162728 0.9183728 ]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>So sad she is a professional prostatiut</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.7292114  0.20363256 0.06715603]</td>\n",
       "      <td>[0.8872626  0.11273739]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Stupid lady , All Armies which allow Gandus ha...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.920077   0.0304047  0.04951836]</td>\n",
       "      <td>[0.78655756 0.2134424 ]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>What a disgrace its not moving anything forwar...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.7686632  0.06361218 0.16772462]</td>\n",
       "      <td>[0.93363357 0.06636649]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Arundhata Roy, uneducated brute. Kungufu kutta.</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.65585417 0.24176931 0.10237651]</td>\n",
       "      <td>[0.9630515  0.03694849]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>I don't care if my SON or DAUGHTER is GAY. It ...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.82572514 0.04576344 0.12851149]</td>\n",
       "      <td>[0.8514948  0.14850527]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>She should be jailed under the law and what sh...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.59657276 0.17583595 0.2275913 ]</td>\n",
       "      <td>[0.99011034 0.00988966]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>We need men to protect us not dick licking idi...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.78090847 0.09617742 0.12291407]</td>\n",
       "      <td>[0.9955005  0.00449949]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>sexy porimoni</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.8349359  0.12494297 0.04012113]</td>\n",
       "      <td>[0.94692266 0.05307735]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>My problem was the way he entered class random...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.9869342  0.00280675 0.01025912]</td>\n",
       "      <td>[9.999411e-01 5.885603e-05]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>That's modafuking hago app ad😂😂</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.8935834  0.05945431 0.0469622 ]</td>\n",
       "      <td>[0.990776   0.00922405]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>Looking at arunditi Roy closely just now she l...</td>\n",
       "      <td>OAG-NGEN</td>\n",
       "      <td>[0.04124348 0.7354514  0.22330508]</td>\n",
       "      <td>[0.76403403 0.23596598]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>Someone should file case against arundhuti (ku...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.85447586 0.07296271 0.07256145]</td>\n",
       "      <td>[0.9943422 0.0056578]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>all homosexuals are paedophiles according to t...</td>\n",
       "      <td>NAG-GEN</td>\n",
       "      <td>[0.7066269  0.11711952 0.1762536 ]</td>\n",
       "      <td>[0.16815427 0.8318457 ]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>Savdhaan india me most of the victims of domes...</td>\n",
       "      <td>NAG-NGEN</td>\n",
       "      <td>[0.5000358  0.20723811 0.2927261 ]</td>\n",
       "      <td>[0.9819723  0.01802776]</td>\n",
       "      <td>NAG-GEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               Text Pred-Label  \\\n",
       "0            0  sorry if i bother somebody.. iam a defence asp...    NAG-GEN   \n",
       "1            1                                 Randi Naxalist Roy   NAG-NGEN   \n",
       "2            2  wow.... india is progressing day by day.... No...   NAG-NGEN   \n",
       "3            3  ARUNDHATI ROY.....HA HA HA........HER PARENTS ...   NAG-NGEN   \n",
       "4            4       Allah and Muhammad can now have sex in India   NAG-NGEN   \n",
       "5            5  World Knows The Prime Roots of Urban NAXAL tha...   NAG-NGEN   \n",
       "6            6  Arundhati real name is Suzan and she is charis...   NAG-NGEN   \n",
       "7            7  We are here because our ancestors were against...    NAG-GEN   \n",
       "8            8  Homosexual hona is not a crime but it shouldn'...   NAG-NGEN   \n",
       "9            9  +onthebookshelf  \\nSexual relationships betwee...   NAG-NGEN   \n",
       "10          10  That's true brother. Seeing feminists butthurt...   NAG-NGEN   \n",
       "11          11  kapil why are u listening to these chutiaassss...   NAG-NGEN   \n",
       "12          12  This is the procedure every where , be it supe...   NAG-NGEN   \n",
       "13          13                                                Hot   NAG-NGEN   \n",
       "14          14                           There are only 2 genders   NAG-NGEN   \n",
       "15          15  She is absolutely xxx movie partner even this ...   NAG-NGEN   \n",
       "16          16  Homosexuality is disgusting. It should be cond...    NAG-GEN   \n",
       "17          17  **We can 't allow to pollute our bravery envir...   NAG-NGEN   \n",
       "18          18  Freedom of expression. Kasthuri it’s bullshit....   NAG-NGEN   \n",
       "19          19  Homosexuality should be allowed in the army......    NAG-GEN   \n",
       "20          20            So sad she is a professional prostatiut   NAG-NGEN   \n",
       "21          21  Stupid lady , All Armies which allow Gandus ha...   NAG-NGEN   \n",
       "22          22  What a disgrace its not moving anything forwar...   NAG-NGEN   \n",
       "23          23    Arundhata Roy, uneducated brute. Kungufu kutta.   NAG-NGEN   \n",
       "24          24  I don't care if my SON or DAUGHTER is GAY. It ...   NAG-NGEN   \n",
       "25          25  She should be jailed under the law and what sh...   NAG-NGEN   \n",
       "26          26  We need men to protect us not dick licking idi...   NAG-NGEN   \n",
       "27          27                                      sexy porimoni   NAG-NGEN   \n",
       "28          28  My problem was the way he entered class random...   NAG-NGEN   \n",
       "29          29                    That's modafuking hago app ad😂😂   NAG-NGEN   \n",
       "30          30  Looking at arunditi Roy closely just now she l...   OAG-NGEN   \n",
       "31          31  Someone should file case against arundhuti (ku...   NAG-NGEN   \n",
       "32          32  all homosexuals are paedophiles according to t...    NAG-GEN   \n",
       "33          33  Savdhaan india me most of the victims of domes...   NAG-NGEN   \n",
       "\n",
       "                              Agg-Prob                       Gen-Prob GT-Label  \n",
       "0   [0.81337935 0.10682596 0.07979469]        [0.20062688 0.79937315]  NAG-GEN  \n",
       "1   [0.96015495 0.02288608 0.01695899]          [0.9624128 0.0375873]  NAG-GEN  \n",
       "2   [0.9327383  0.02143447 0.04582729]        [0.99649435 0.00350569]  NAG-GEN  \n",
       "3   [0.84490246 0.03739336 0.11770418]        [0.9980465  0.00195348]  NAG-GEN  \n",
       "4   [0.90352356 0.02240491 0.07407154]        [0.9886021  0.01139788]  NAG-GEN  \n",
       "5   [0.9047116  0.03067101 0.06461739]        [0.99661404 0.00338593]  NAG-GEN  \n",
       "6   [0.9034141  0.04429049 0.05229545]        [0.9945122  0.00548781]  NAG-GEN  \n",
       "7   [0.7449015  0.08207034 0.17302823]        [0.09623028 0.90376973]  NAG-GEN  \n",
       "8   [0.95881957 0.01758637 0.02359411]        [0.9765638  0.02343618]  NAG-GEN  \n",
       "9   [0.8022871  0.05470058 0.14301236]        [0.8259745  0.17402545]  NAG-GEN  \n",
       "10  [0.4651227  0.25265834 0.28221893]        [0.9833097  0.01669028]  NAG-GEN  \n",
       "11  [0.57544947 0.19784397 0.22670656]        [0.98515826 0.01484168]  NAG-GEN  \n",
       "12  [0.8943346  0.03550828 0.07015709]        [0.5750906  0.42490944]  NAG-GEN  \n",
       "13  [0.991954   0.00293526 0.00511077]  [9.9975806e-01 2.4197616e-04]  NAG-GEN  \n",
       "14  [0.4688868  0.25112578 0.27998734]        [0.60404044 0.39595956]  NAG-GEN  \n",
       "15  [0.9315344  0.03082788 0.03763773]        [0.98365027 0.01634971]  NAG-GEN  \n",
       "16  [0.55568665 0.2614945  0.18281882]        [0.11387955 0.88612044]  NAG-GEN  \n",
       "17  [0.7579064  0.0876889  0.15440473]        [0.9662474  0.03375266]  NAG-GEN  \n",
       "18  [0.5485263  0.04553279 0.40594092]        [0.99061877 0.00938131]  NAG-GEN  \n",
       "19  [0.83159167 0.08601627 0.08239194]        [0.08162728 0.9183728 ]  NAG-GEN  \n",
       "20  [0.7292114  0.20363256 0.06715603]        [0.8872626  0.11273739]  NAG-GEN  \n",
       "21  [0.920077   0.0304047  0.04951836]        [0.78655756 0.2134424 ]  NAG-GEN  \n",
       "22  [0.7686632  0.06361218 0.16772462]        [0.93363357 0.06636649]  NAG-GEN  \n",
       "23  [0.65585417 0.24176931 0.10237651]        [0.9630515  0.03694849]  NAG-GEN  \n",
       "24  [0.82572514 0.04576344 0.12851149]        [0.8514948  0.14850527]  NAG-GEN  \n",
       "25  [0.59657276 0.17583595 0.2275913 ]        [0.99011034 0.00988966]  NAG-GEN  \n",
       "26  [0.78090847 0.09617742 0.12291407]        [0.9955005  0.00449949]  NAG-GEN  \n",
       "27  [0.8349359  0.12494297 0.04012113]        [0.94692266 0.05307735]  NAG-GEN  \n",
       "28  [0.9869342  0.00280675 0.01025912]    [9.999411e-01 5.885603e-05]  NAG-GEN  \n",
       "29  [0.8935834  0.05945431 0.0469622 ]        [0.990776   0.00922405]  NAG-GEN  \n",
       "30  [0.04124348 0.7354514  0.22330508]        [0.76403403 0.23596598]  NAG-GEN  \n",
       "31  [0.85447586 0.07296271 0.07256145]          [0.9943422 0.0056578]  NAG-GEN  \n",
       "32  [0.7066269  0.11711952 0.1762536 ]        [0.16815427 0.8318457 ]  NAG-GEN  \n",
       "33  [0.5000358  0.20723811 0.2927261 ]        [0.9819723  0.01802776]  NAG-GEN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fd95f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04124348, 0.7354514, 0.22330508]]\n",
      "[-0.69420792, -0.1820616]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.43813475999999996"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseNoagg(df_NG)\n",
    "print(x)\n",
    "print(y)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216d03f",
   "metadata": {},
   "source": [
    "## Gender Inference Analysis - Matching Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7f5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Inference files\n",
    "df_OG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_OG.csv\")\n",
    "df_CG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_CG.csv\")\n",
    "df_NG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_NG.csv\")\n",
    "df_ONG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_ONG.csv\")\n",
    "df_CNG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_CNG.csv\")\n",
    "df_NNG = pd.read_csv(\"/Users/amit1.bhatti/Desktop/AI-LAB/experiment/Thesis_work/TRAC-2-1/preprocessing/model_outputs/dev-prediction/pred_dev_NNG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b3e559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseGenMatch(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"-GEN\" in x)]\n",
    "    #corr_df = corr_df[corr_df[\"Pred-Label\"] == corr_df[\"GT-Label\"]]\n",
    "    #corr_df = df[(df[\"Pred-Label\"].apply(lambda x : \"GEN\" in x)) & (df[\"Pred-Label\"] == df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Gen-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        \n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    \n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='NonGendered')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Gendered')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Gen-Match.png\")\n",
    "        \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c5c61ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6509483911111111"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Gen = pd.concat([df_OG, df_CG, df_NG])\n",
    "x, y = predAnalyseGenMatch(df_Gen)\n",
    "print(len(x))\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae701fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13846153846153847"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9/65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b98644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseGenNMatch(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NGEN\" in x)]\n",
    "    #corr_df = corr_df[corr_df[\"Pred-Label\"] != corr_df[\"GT-Label\"]]\n",
    "    #corr_df = df[(df[\"Pred-Label\"].apply(lambda x : \"GEN\" in x)) & (df[\"Pred-Label\"] != df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Gen-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        \n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    \n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='NonGendered')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Gendered')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, fontsize=7)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/Gen-NonMatch.png\")\n",
    "        \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34062e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8402009863008928"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Gen = pd.concat([df_OG, df_CG, df_NG])\n",
    "x, y = predAnalyseGenNMatch(df_Gen)\n",
    "print(len(x))\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aefaf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseNGenMatch(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NGEN\" in x)]\n",
    "    #corr_df = corr_df[corr_df[\"Pred-Label\"] == corr_df[\"GT-Label\"]]\n",
    "    # corr_df = df[(df[\"Pred-Label\"].apply(lambda x : \"NGEN\" in x)) & (df[\"Pred-Label\"] == df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Gen-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        \n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    \n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(30,12))\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='NonGendered')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Gendered')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/NGen-Match.png\")\n",
    "        \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a18d37bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8568549909159289"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NGen = pd.concat([df_ONG, df_CNG, df_NNG])\n",
    "x, y = predAnalyseNGenMatch(df_NGen)\n",
    "print(len(x))\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e37f61c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9662162162162162"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "858/888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "582952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predAnalyseNGenNMatch(df):\n",
    "    \n",
    "    # Filter the Data point there the GT-Label == Pred-Label\n",
    "    corr_df = df[df[\"Pred-Label\"].apply(lambda x : \"NGEN\" not in x)]\n",
    "    # corr_df = corr_df[corr_df[\"Pred-Label\"] != corr_df[\"GT-Label\"]]\n",
    "    #corr_df = df[(df[\"Pred-Label\"].apply(lambda x : \"NGEN\" in x)) & (df[\"Pred-Label\"] != df[\"GT-Label\"])]\n",
    "    # For each of the datapoint, take the difference of the probability value for Overtly and Covertly\n",
    "    list_scores = []\n",
    "    list_diffs = []\n",
    "    \n",
    "    for probs in corr_df[\"Gen-Prob\"]:\n",
    "        probs = probs.replace(\"[\", \"\")\n",
    "        probs = probs.replace(\"]\", \"\")\n",
    "        probs = np.fromstring(probs, dtype=float, sep=\" \")\n",
    "        \n",
    "        # Storage format (class_focus, target_class, class_focus - target_class)\n",
    "        list_scores.append([probs[0], probs[1]])\n",
    "        list_diffs.append(probs[0] - probs[1])\n",
    "        \n",
    "    # Plot OC\n",
    "    x = np.arange(1, len(list_scores)+1)\n",
    "    y1 = [val[0] for val in list_scores]\n",
    "    y2 = [val[1] for val in list_scores]\n",
    "    \n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25,12))\n",
    "    rect1 = ax.bar(x - width/2, y1, width=width, label='NonGendered')\n",
    "    rect2 = ax.bar(x + width/2, y2, width=width, label='Gendered')\n",
    "\n",
    "    ax.set_ylabel('Probabilites')\n",
    "    ax.set_title('Probabilities Comparison Classes')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(\"./plots/Probability-Confidence/NGen-NMatch.png\")\n",
    "        \n",
    "    return list_scores, list_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d95c847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.42890334599999996"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = predAnalyseNGenNMatch(df_NGen)\n",
    "print(len(x))\n",
    "np.mean(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
